\subsection{GDPD}
The Global Data Plane daemon (GDPd) is a background process that runs on GDP servers. It forms the foundation of our system and handles the physical storage of logs and their entries. We use a previously created prototype implementation of this component. Currently, logs are stored as a flat file in which entries are stored in a contiguous sequence. Additionally, an index is maintained which maps log entries to file offsets. Thus, accessing a specific log entry entails looking up its offset using the entry number in the index, seeking to this offset within the flat file, and finally reading the entry. The GDP daemon features a standard client-server interface based on TCP. Clients who wish to use the daemon must issue commands conforming to a specific protocol.

\subsection{GDP Lib}
The GDP Library (Lib for short) makes it easy for users to interact with a running GDPd instance in their code. The library handles all of the underlying networking needed to communicate with the daemon via TCP. As with GDPd, we use a previously created prototype implementation of GDP Lib. This prototype implementation is intended for use in C code, although other language bindings have been created as well. We specifically make use of the following functions:
\begin{enumerate}
\item \texttt{gdp\_gcl\_create} initializes a new log
\item \texttt{gdp\_gcl\_open} opens an existing log, either for reading or appending
\item \texttt{gdp\_gcl\_read} reads a specific entry from a log
\end{enumerate}

\subsection{GDP-Wrapper}
We tie GDPD and GDP Lib together in an object-oriented C++ wrapper library. This wrapper library is intended to provide a cleaner abstraction than the combination of GDPd and GDP Lib, which now must be run together on a GDP server. While this violates the original design intent of the two components, we found it far easier to rely on the simple function calls of GDP Lib rather than the protocol of GDPd to perform log storage. In addition, this approach had two important benefits. First, it expedited our development process. Second, it kept our distributed GDP prototype isolated from changes in the GDPd protocol or its implementation.

The primary abstraction of the GDP-Wrapper library is the GDP Channel Log (GCL) object, which features a very simple interface. Instantiating the object will automatically open a GDP log with the provided name or create the log if it did not previously exist. The object then exposes simple \texttt{read} and \texttt{append} methods that allow the underlying log to be accessed and modified.

\subsection{Chimera}
Chimera is an open source library that provides a C implementation of location independent routing. It achieves this using a prefix routing protocol similar to the one used by Tapestry.  Each node's routing information consists of two data structures: a leaf set and routing table. The leaf set contains the current node's immediate neighbors in key space. When a message arrives that needs to be routed, the local node checks its leaf set first. If the destination falls in the leaf set range, it will be forwarded to the closest node in the leaf set. Otherwise, it will be routed through the routing table to the node with the longest common prefix with the destination.
 
Figure 1 provides a simplified system level diagram of Chimera. The network layer of Chimera is the layer, which physically transmits raw data on the wire and receives incoming bytes from the wire. It sends the bytes it receives to the message layer, which takes this raw data and converts it into the proper Chimera message format. Messages exchanged in chimera mainly consist of a message type, a destination key, a source key and the message payload. The type field specifies the purpose of the message such as an append request, a read request, an append acknowledgement, join request etc. The destination key field provides the unique identifier of the recipient of the message whereas the source key provides the identifier of the node generating the message. Based on the message type, each message at the receiving node is handed to an up-call in the routing system, which does the required processing and alerts the application on top of it via an up-call if necessary. This approach is used to implement an event driven message-passing system. On initialization, chimera creates a number of threads, which form a thread pool. Messages arriving at a given node are placed in a job queue. A free thread from the thread pool removes a message from the queue and passes it to the appropriate up-call. Once the processing is done, the thread returns to the thread pool. If the application generates a message, Chimera will perform a route lookup using its routing information (leafset and routing table) and send the message back to the message layer along with the physical location of the receiving node at the next hop of the message. The message layer converts this message into a stream of bytes that is sent to the network layer to physically transfer the message.
 
\subsubsection{Joining Of New Nodes in the Network}
The mechanism for Joining of a node in the chimera network works as follows. The joining node would send a message of type \texttt{CHIMERA\_JOIN} to a bootstrap node whose physical location is known to the joining node. The bootstrap node forwards the request to the peer whose key matches closest to that of the joining node in its leafset. This node would then send a message of type \texttt{CHIMERA\_JOIN\_ACK} back to the joining node. In this message, it will include the physical location of itself and all the nodes in its leafset (part of the message payload). On receiving this message, the new joining node would look at all the nodes included in the message and update its routing information with location of the nodes. It will then send a CHIMERA\_UPDATE message to each of these nodes to notify its presence in the network. On receipt of this message, each node will update its own routing information to account for the new node. Figure 2 shows the flow of messages between the new joining node, the bootstrap node and the peer processing the join request.
 
\subsubsection{Pinging of Nodes}
In order to ensure network consistency, each node determines if each of its neighboring leafset nodes are still in the network. This is achieved through a periodic pinging mechanism that is handled by a separate thread running on each node in the network. The thread sends a message of type \texttt{CHIMERA\_PING} to each of the nodes in the given peer’s leafset at regular intervals. The neighbors on receiving this message send an acknowledgement back to the peer indicating their presence in the network. If an acknowledgment is not received within a specified time-out interval, the node is removed from the peer’s leafset and routing table.

\subsection{Chimera Routing Manager}
We implemented an object oriented C++ wrapper around the Chimera open source library, called the Chimera Routing Manager. The role of this wrapper library is to essentially translate Chimera’s existing C interface into a compatible C++ interface that can be used by the layers above such as the DLog and HTTP layers. The library consists of a single class called ChimeraRoutingManager. Some of the important member subroutines of the class are as follows:
 \begin{itemize}
\item \texttt{void JoinNetwork(host, port)} : This subroutine invokes the Chimera Join function to add its caller to the chimera network. It receives the location of the bootstrap node from the layers above (JoinHost and JoinPort) and sends a message to of type \texttt{CHIMERA\_JOIN} to it.
 \item \texttt{void AddMessageType(type, ack)}: This function is used to associate Chimera’s delivery up-call with six different types of messages that are exchanged between the GDP nodes.
 \item \texttt{void SendMessage(key, type, size, data)} : This function is used to send messages between the GDP nodes. It first creates a message packet with the destination key as \texttt{key}, message type \texttt{type}, message size \texttt{size} and message payload as \texttt{data}
 \end{itemize}
 
 We define six types of messages that are circulated within the Chimera network:
 \begin{enumerate} 
\item \texttt{APPEND\_LOG}: Request for appending the content of the message payload into the Log whose hash(UUID) is represented by the message destination key.
\item \texttt{READ\_LOG}:  Request for reading the content of the Log whose hash (UUID) is represented by the message destination key.
\item \texttt{APPEND\_LOG\_ACK}: Send the number of bytes appended into a log when the corresponding message of type \texttt{APPEND\_LOG} is received.
\item \texttt{READ\_LOG\_ACK}: Send the content of the log that is addressed by a corresponding message of type \texttt{READ\_LOG}.
\item \texttt{APPEND\_LOG\_NACK}: Send an error that occurred while appending into a log when the corresponding\\\texttt{APPEND\_LOG} message is received
\item \texttt{READ\_LOG\_NACK}: Send an error in reading from the log that is addressed by a corresponding message of type \texttt{READ\_LOG}
\end{enumerate}

\subsection{DLog}
The purpose of the DLog layer is to integrate Chimera and GDP by receiving requests from its clients, routing them through Chimera and finally using the GDP framework to processes them. It provides two subroutines to its clients:
 \begin{itemize}
\item \texttt{int append(UUID, data, size)} : This subroutine allows the DLog clients to issue append requests to  a log whose ID is specified by the UUID. The data parameter specifies that data to be appended to the log and size provides the number of bytes to be appended.
 \item \texttt{int read(UUID, record\_num, buffer,  size)}: This subroutine allows DLog clients to issue a read request, to read data in the form of record (specified by \texttt{record\_num}) from a log specified by the UUID. The data read from the log is placed inside the buffer.
 \end{itemize}
 
In our design, we currently couple routing of messages with the storage. The DLog layer performs this coupling through the following mechanism. It receives a read/append request from its client. The request includes the Log ID on which the read/append needs to be performed. The DLog layer computes the SHA-256 hash of this ID to produce a unique object key. The request is then forwarded to the Chimera layer in the form of a Chimera message with the destination key as the object key. The node whose ID matches closest to this object key receives the request and is said to be the owner of the log to which the read/append request is addressed. The DLog layer, running on this receiving node, forwards the request to the GDP layer which performs the operation specified by the request. It then generates the corresponding acknowledgment message and routes it back to the requesting node through Chimera.
 
We have implemented DLog to support multithreading. This enables multiple clients to connect to the DLog layer concurrently. However, at this point, we have made the client interaction with the DLog layer to be a synchronous form of communication. When a client makes a call to read or append, a request is placed on a request queue. A record of the pending operation is also stored, and the client blocks on a condition variable associated with this record. The client will be blocked from executing further until the DLog layer receives an acknowledgment for its request. The following subroutines are used to process the queries placed in the request queue. DLog runs several worker threads in a pre-allocated pool. These threads continuously query the request queue for new work in this subroutine. After removing a request from the queue, a worker thread creates the proper Chimera message and sends the message. The thread then grabs another request from the queue and repeats the process.
 
Dlog must also handle incoming Chimera messages. When the message received is a read or append request, the function forwards the request to the GDP layer for processing and sends the corresponding acknowledgment message back to the Chimera layer in order to route it back to the source. When the message received is a response to a read or append request, the result of the request (an integer return value and possibly payload data) is extracted from the response message and returned to the client, which is unblocked via a signal to the corresponding condition variable.