\subsection{Tuning Leafset Size}
Our first experiment towards enhancing the efficiency of routing in terms of latency was to vary the size of the leafset in each Chimera node. The Chimera Library provides a macro called \texttt{LEAFSET\_SIZE} that can be varied with the constraint that the size should be an even number. This is because Chimera splits the entire leafset into two arrays, Left leafset and Right Leafset, each having a size \texttt{LEAFSET\_SIZE/2}
 
We initially disabled leafset caching to obtain our base case where all the nodes are forced to use the P2P overlay network to discover the destinations of their messages. Although this optimization doesn’t contribute towards improving the network latency of our routing system, it does help us set up a baseline over which we could compare our other optimizations. This approach also helps to determine if the system performs reasonably well in a memory-constrained environment.
 
We next increased the leafset size to accommodate all the nodes in the network in a given node’s leafset. This would result in a one hop DHT architecture where messages take at most one hop to reach from source to their destinations. Ideally this case is in feasible in practical system design due to memory constraints. Nevertheless, this experiment provided us with the best-case scenario.

\subsection{Cache Replacement Policies}
Our next experiment was to improve the type of caching provided by Chimera in a node’s leafset. By default, Chimera’s replacement policy in the case when the leafset cache becomes full was to simply remove the very last entry in the cache. This policy has no relation with the popular nodes in the leafset to which messages are routed frequently. In order to improve leafset caching, we replaced this policy with three cache replacement policies
\begin{enumerate}
\item Least Recently Used (LRU): We modified the cache to add unique sequence number in addition to the physical location of a node in each entry of the leafset. This number is incremented every time a message is forwarded to the corresponding node entry.  When the cache becomes, the entry with the smallest sequence number is removed to make room for the new entry.
 \item First In First Out (FIFO): For this policy, we recorded the time when a given entry was placed in the cache, in addition to the physical location of a node. When the cache becomes full, the entry with the earliest timestamp is removed to make room for the new entry.
 \item First Come First Served (FCFS): This was the simplest to implement as the policy just ignores any new entry to be added into the leafset cache when it becomes full.
 \end{enumerate}
 
\subsection{Direct Source Optimization}
Our third optimization, which we have called \textit{Direct Source Optimization}, involved reducing the number of overlay hops taken by messages in Chimera by exposing the physical location of the peers. When a sender generates a read/append request, Chimera routes the message through multiple hops until it finds the recipient whose key matches closest to the SHA-256 hash of the Log ID included in the request.
 
In order to implement the optimization, we also included the sender’s physical location in the form of the host name and the port number inside the request. Thus when the request arrives at the receiving end, the node can compute the IP address of the sender. This way it can send the corresponding acknowledgment directly to the sender using the IP layer, without having to route it through multiple hops using Chimera. The next time when the sender wants to send a request for a log, which resides in the same receiving node, the sender would know the receiver’s physical location as well. This is because the receiver node will also include its physical location in the first acknowledgment message it sends to the source of the request. The source will update its leafset with this new location.  
 
Figures 3a describes an example of routing of an append request by Chimera originally. The message path taken by an Append Request is routed from a source node with key b64a to a destination whose key is 3645 using multiple hops. Without the optimization, the corresponding acknowledgement message sent by the destination also takes multiple hops to reach the source. The append and the corresponding acknowledgment message contain the same sequence number. Figure 3b shows that when next time the source node generates an append request for the same destination, Chimera uses the same paths with multiple hops for routing both the messages.
 
Figure 4a next describes the same example after employing our optimization. The first time when the append request is sent, it takes multiple hops to reach the destination. However since we have included the source’s physical location inside the message, the receiving node has enough information to send the corresponding acknowledgment directly to the source. When this acknowledgment message arrives, the source will also receive the physical location of the node with key 3645, which it will store in its leafset cache. Figure 4b shows that next time when an append request is generated for the same destination node, the message will be sent directly to the node by the source.
 
The motivation behind this optimization is that we are trying to eventually converge to a one hop DHT where each source node is aware of the physical locations of all its popular destination nodes. This method allows us to make use of the IP layer for popular paths in the network. A tradeoff for this optimization is that although we reduce the total round trip time for serving of a request, we have increased the message sizes. Hence we have improved upon latency at the cost of the network bandwidth.
