\subsection{Peer-to-Peer Overlay Networks}
An overlay network is a virtual network topology layer typically on top of the physical IP network. All nodes participating in this overlay are known as peers. These peers provide a distributed system architecture, which is self-organizing and fully decentralized. Unlike the traditional client-server architecture, all peers in the network are considered equal. They share bandwidth, storage and computation. Each peer can act as a client consuming resources from the system and at same time be a server providing resources to the network. Together, these nodes form a self-scaling peer-to-peer (P2P) network.

The first generation of P2P systems included Napster and Gnutella for file sharing and storage applications. Napster used a centralized client-server architecture where locations of files were stored in a centralized directory. The main limitation of this architecture was scalability to due a single point of failure. Gnutella used a flooding mechanism to route messages from a given source node to a destination node. This approach used a iterative process where the source node with query its neighboring nodes for the location of the destination node. The neighboring nodes would query their neighbors and so on until the destination node is located. This technique doesn’t scale well because of the bandwidth and processing requirements they place on the network.

The second generation of P2P systems are structured overlay networks including Chord and Tapestry. Essentially, these overlays use consistent hashing to provide Key-Based Routing in which objects (which could be file chunks) and nodes are assigned unique identifiers from the same namespace called keys. Messages addressed to any key will incrementally route towards an overlay node responsible for that key \cite{chord}. A main drawback of Chord and other similar architectures such as CAN is that these systems don’t consider physical network distances when constructing the overlay. Thus a given overlay hop may span across the diameter of the network \cite{tapestry}. 

Tapestry constructs locally optimized routing table during initialization and maintains them to reduce routing stretch.  It provides a high-performance, scalable, and location-independent routing of messages using only localized resources. The algorithm uses local tables at each node, called neighbor maps, to route overlay messages to the destination ID digit by digit similar to the IP prefix matching protocol. The algorithm also maintains object pointers on each node to reduce the number of hops while looking for the given object. These pointers are stored during the publishing phase, when a given node advertises a specific object it stores, by routing a publish message towards the object’s root node (the node whose key matches the closest to the object key in the network). Each node along the publication path stores a pointer mapping that specifies the location of the object. If any of these nodes receive a lookup message for this object, they forward the message directly to the object owner node.  Using this approach, the algorithm maintains locality.

Although Tapestry performs well in a global system, its performance degrades in a local network where multiple overlay hops could be replaced with a single physical hop in the IP layer. Thus the goal of our system is to improve Tapestry by combining it with the physical layer to provide a more optimized routing algorithm.

\subsection{Large-Scale Storage Systems}
Numerous storage systems have been devised that operate at a large scale. Google \cite{bigtable, megastore}, Amazon \cite{dynamo} and Facebook \cite{haystack} have all developed software platforms that act as interfaces to the large collections of commodity servers contained in their data centers. While these systems are distributed in that they contain large quantities of nodes, and may even operate across geographically distant locations, they embody the cloud-based approach described earlier. Microsoft's Bolt \cite{bolt}, on the other hand, is essentially a hybrid system that incorporates both cloud-based storage resources as well as at-home servers. These local servers act as caches and enable continued operation even when a connection to the cloud is lost. We envision the Global Data Plane incorporating many important ideas from these systems, such as replication for fault tolerance and protocols to enforce agreement amongst these replicas. However, the Global Data Plane is intended to consist of nodes that are more geographically distributed than those seen in these systems. This structure is similar to what was produced by the OceanStore \cite{oceanstore} project, and the GDP's use of logs is specifically inspired by the follow-on Antiquity \cite{antiquity} project.

\subsection{Sensor Networks}
Sensor networks have become pivotal in the contemporary notion of “Internet of Things(IoT)”. It is a widely researched field with continuing advances which shows a lot of innovation potential. Sensor networks have been extensively used for environmental monitoring and human health monitoring systems and have been successfully deployed in many past projects \cite{ducks}. The notion of using sensor networks in building automation systems has gained popularity over the recent past. As these systems begin to scale, we are increasingly faced with challenges associated with managing large-scale time-series data. Projects, like sMAP from UC Berkeley \cite{smap} and Respawn \cite{respawn} from CMU, use customized datastores that provide an optimized performance for specific applications. Unfortunately scaling, sub-second time-stamping and access control are often parameters that are neglected parameters due to the limited applicability of the datastores. The GDP overcomes these issues by providing capabilities to perform per-stream permissions and distribute and query data across multiple local and remote servers.