Our work is primarily motivated by the ongoing Global Data Plane (GDP)\cite{gdp} project at Berkeley's Swarm Lab. The GDP aims to provide a unified network and data storage platform for sensors and actuators. The system will be capable of dealing with sensor data (both in raw and processed forms) as well as actuation signals and will offer publish-subscribe semantics to its clients. Other important goals for the GDP include durability of data through replication and erasure coding, scaling to a large quantity, say $10^{12}$, of nodes, and seamless migration of data between servers when necessary. In particular, security is considered one of the foremost concerns in the GDP. We plan to use cryptographic tools like signatures to maintain data integrity and public key cryptography to maintain privacy.

The Global Data Plane is particularly relevant considering the recent explosion of interest in the Internet of Things (IoT). Research projects \cite{homeOS, terraswarm, bas, boss} have studied new potential IoT systems, new products such as \cite{arduino} have tapped into the ``maker'' movement, and large tech companies such as Google have begun investing in IoT technology \cite{nest}. However, the IoT movement remains lacking in cohesive storage platforms for its applications. The state of the art, as evidenced by several examples such as \cite{ibm}, remains centered on cloud computing and storage, depriving users of the many potential benefits of locality. The Global Data Plane, on the other hand, is centered on the use of fog computing \cite{fog} resources. By incorporating resources that are closer to network endpoints (particularly sensors and actuators), we believe that users will see improvements in areas such as latency and data durability compared to a cloud-centric model.

The intent is for the Global Data Plane to be deployed on a large, geographically distributed collection of heterogeneous nodes. Sensors and actuators will communicate with nearby nodes, potentially acting as gateways, and data can in principle be placed on any storage server within the system depending on the user's needs. We imagine that a variety of storage options will be available, ranging from powerful servers running in the cloud to more modest machines running in local area networks and co-located with sensors and actuators. In order to achieve this vision, an efficient, a robust, and flexible means of routing between GDP servers must be implemented. Such a routing mechanism is the focus of this work.

\textit{Nikhil: Discussion of Location-Independent Routing and its Benefits}

We have implemented a distributed prototype of the Global Data Plane featuring location-independent routing between storage servers. In particular, we expose an append-only log as the primary storage interface to users. Thus, users may append to a particular log, or they may read a specific entry from a specific log. We leave the implementation of publish-subscribe semantics to future work. We use consistent hashing, provided by the Chimera\cite{chimera} library, to assign responsibility for a particular log to a particular server based on the name of that log, achieving the desired location-independent routing described above. In our system, a server is expected to directly store the logs for which it is responsible, but we plan to change this such that the server is only responsible for knowing the location of a log. While this adds an extra layer of indirection to the system, it is necessary for intelligent log placement and migration.

Moreover, we have improved upon the basic functionality of Chimera, which is purely a peer-to-peer overlay network. Specifically, we have augmented the overlay network with simple optimizations based on physical network topology. These optimizations are intended to reduce the inefficiencies that are inherent in an overlay network scheme, such as links between peers that span  large physical distances. This can lead to routes that unnecessarily cover a large physical distance when a more direct route would be possible given knowledge of the underlying network.

Finally, we evaluate and analyze the effectiveness of our routing scheme and its optimizations using a group of geographically distributed GDP storage servers deployed in the cloud and a large collection of simulated sensor clients. A client must have knowledge of a bootstrap node to use our GDP network, but discovery is a natural direction for future work. We use latency as the primary means of measuring performance, and we measure it in several different forms. This includes the end-to-end latency observed by the simulated clients as well as network latency associated only with the routing of requests among the GDP servers.

\textit{High-level discussion of results}
